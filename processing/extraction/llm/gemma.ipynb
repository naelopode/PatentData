{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8f77877e",
   "metadata": {},
   "source": [
    "This is used for benchmark purposes, not a optimized pipeline for llm extraction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2be33399",
   "metadata": {},
   "source": [
    "This is a two stage llm pipeline, we first extract key information and then transform to a json format. This allows a performance boost."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "def50d8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install accelerate\n",
    "import os\n",
    "os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"expandable_segments:True\"\n",
    "\n",
    "from transformers import AutoProcessor, Gemma3ForConditionalGeneration\n",
    "from PIL import Image\n",
    "import requests\n",
    "import torch\n",
    "\n",
    "model_id = \"google/gemma-3-4b-it\"\n",
    "\n",
    "model = Gemma3ForConditionalGeneration.from_pretrained(\n",
    "    model_id, device_map=\"auto\"\n",
    ").eval()\n",
    "\n",
    "processor = AutoProcessor.from_pretrained(model_id)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5a61b7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pymongo import UpdateOne\n",
    "from tqdm import tqdm\n",
    "from pymongo import MongoClient\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from datetime import datetime\n",
    "\n",
    "client = MongoClient(\"localhost\", 29012)\n",
    "db = client[\"test-database\"]\n",
    "collection_json = db[\"collection-json\"]\n",
    "collection_txt = db[\"collection-txt2\"]\n",
    "collection_CNN = db[\"CNN_DE\"]\n",
    "\n",
    "def get_text(publication_number):\n",
    "    publication_number = f\"{publication_number[:2]}.{publication_number[2:-1]}.{publication_number[-1:]}\"\n",
    "    pipeline = [\n",
    "        {\"$match\": {\"Publication Number\": publication_number}},  # Filter documents where 'type' is 'text'\n",
    "        {\"$group\": {\n",
    "            \"_id\": \"$Publication Number\",  # Group by 'Publication Number'\n",
    "            \"pages\": {\"$push\": \"$page\"},   # Collect 'page' values into an array\n",
    "            \"text\": {\"$push\": \"$OCR\"}  # Collect 'OCR' values into an array\n",
    "        }}\n",
    "    ]\n",
    "\n",
    "    result_db = list(collection_CNN.aggregate(pipeline))\n",
    "    if not result_db:\n",
    "        return None\n",
    "    if len(result_db[0][\"text\"]) == 0:\n",
    "        return None\n",
    "    result ={\n",
    "        \"Publication Number\": result_db[0][\"_id\"],\n",
    "        \"pages\": result_db[0][\"pages\"],\n",
    "        \"OCR\": ' '.join([item for item in result_db[0][\"text\"]])  # Join the text from all pages\n",
    "    }\n",
    "    return result\n",
    "\n",
    "def generate_query(item):\n",
    "    if not isinstance(item, dict):\n",
    "        print(item)\n",
    "        raise ValueError(\"Expected 'item' to be a dictionary.\")\n",
    "    \n",
    "    if 'OCR' not in item:\n",
    "        raise KeyError(\"'OCR' key is missing in the item dictionary.\")\n",
    "    \n",
    "    if isinstance(item['OCR'], str):\n",
    "        text_clean = item['OCR'].replace('\\n', ' ').strip()\n",
    "        return text_clean\n",
    "    if isinstance(item['OCR'], bytes):\n",
    "        text_clean = item['OCR'].decode('utf-8').replace('\\n', ' ').strip()\n",
    "        return text_clean\n",
    "    \n",
    "    raise TypeError(\"'OCR' must be either a string or bytes.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e2fb139",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List, Optional\n",
    "from pydantic import BaseModel\n",
    "import datetime\n",
    "class Patent(BaseModel):\n",
    "    title : str\n",
    "    Application_Date: Optional[datetime.datetime]\n",
    "    Publication_Date: Optional[datetime.datetime]\n",
    "    KeyEntity: list[str]\n",
    "    #Inventors: list[str]\n",
    "\n",
    "    class Config:\n",
    "        schema_extra = {\n",
    "            \"additionalProperties\": False,\n",
    "            \"json_encoders\": {\n",
    "                datetime.datetime: lambda v: v.isoformat()\n",
    "            }\n",
    "        }\n",
    "# serialize pydantic model into json schema\n",
    "pydantic_schema = Patent.schema_json()\n",
    "\n",
    "prompt = f\"You are a helpful assistant that transform historical scans of patents to a JSON format. Make sure to get the dates and names correct and only include keys 'title', 'Application_Date', 'Publication_Date', 'KeyEntity' with the names of Applicants, Inventors and Assignees. Take a second to think about your answer. Here's the json schema you must adhere to:\\n{pydantic_schema}\\n\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ac53ae5",
   "metadata": {},
   "outputs": [],
   "source": [
    "item = collection_json.find_one({'Country':\"US\", 'OCR': {'$exists': True}, 'Title': {'$exists': True}, 'C_Application Date': {'$exists': True}, 'C_Publication Date': {'$exists': True}, 'clean_applicants': {'$exists': True}, 'clean_inventor': {'$exists': True}})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0ac53ae5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def first_stage(item):\n",
    "    og_query = generate_query(item)\n",
    "    if len(og_query) > 40000:\n",
    "        og_query = og_query[:34000] + \" \" + og_query[-1000:]\n",
    "    messages = [\n",
    "        {\n",
    "            \"role\": \"system\",\n",
    "            \"content\": [{\"type\": \"text\", \"text\": \"Identify the patent title, application date, publication date, applicants, inventors and assignee as key entity (people or company) from the text. If data is missing, interpret it based on the information you have, values can be None. Please don't add any comments or explanations.\"}]\n",
    "        },\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": [\n",
    "                {\"type\": \"text\", \"text\":  og_query},\n",
    "            ]\n",
    "        }\n",
    "    ]\n",
    "    print(f\"Length of query is {len(generate_query(item))}\")\n",
    "    inputs = processor.apply_chat_template(\n",
    "        messages, add_generation_prompt=True, tokenize=True,\n",
    "        return_dict=True, return_tensors=\"pt\"\n",
    "    ).to(model.device, dtype=torch.bfloat16)\n",
    "\n",
    "    input_len = inputs[\"input_ids\"].shape[-1]\n",
    "    with torch.inference_mode():\n",
    "        generation = model.generate(**inputs, max_new_tokens=150, do_sample=False)\n",
    "        generation = generation[0][input_len:]\n",
    "\n",
    "    decoded = processor.decode(generation, skip_special_tokens=True)\n",
    "    del inputs\n",
    "    del generation\n",
    "    \n",
    "    return decoded#.replace('assignee', 'applicant').replace('Assignee', 'Applicant')\n",
    "\n",
    "def second_stage(decoded, prompt):\n",
    "    messages = [\n",
    "        {\n",
    "            \"role\": \"system\",\n",
    "            \"content\": [{\"type\": \"text\", \"text\": prompt}]\n",
    "        },\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": [\n",
    "                #{\"type\": \"image\", \"image\": \"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/bee.jpg\"},\n",
    "                {\"type\": \"text\", \"text\": decoded},\n",
    "            ]\n",
    "        }\n",
    "    ]\n",
    "\n",
    "    inputs = processor.apply_chat_template(\n",
    "        messages, add_generation_prompt=True, tokenize=True,\n",
    "        return_dict=True, return_tensors=\"pt\"\n",
    "    ).to(model.device, dtype=torch.bfloat16)\n",
    "\n",
    "    input_len = inputs[\"input_ids\"].shape[-1]\n",
    "    with torch.inference_mode():\n",
    "        generation = model.generate(**inputs, max_new_tokens=300, do_sample=False)\n",
    "        generation = generation[0][input_len:]\n",
    "\n",
    "    decoded = processor.decode(generation, skip_special_tokens=True)\n",
    "    del inputs\n",
    "    del generation\n",
    "    \n",
    "    return decoded"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "838f079f",
   "metadata": {},
   "source": [
    "For txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "159f3ee1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "import json_repair\n",
    "import json\n",
    "import random\n",
    "query = {'Country': \"DE\"}\n",
    "training_data = []\n",
    "total_count = collection_txt.count_documents(query)\n",
    "random_indexes = random.sample(range(total_count), 500)\n",
    "for item in tqdm(random_indexes):\n",
    "    item = collection_txt.find_one({'Country': \"DE\"}, skip=item)\n",
    "    publication_number = item['Publication Number']\n",
    "    json_ocr = get_text(publication_number)\n",
    "    if json_ocr is None:\n",
    "        print(f\"Publication Number {item['Publication Number']} not found\")\n",
    "        continue\n",
    "    summary = first_stage(json_ocr)\n",
    "    print(\"---- summary ----\")\n",
    "    print(summary)\n",
    "    json_llm = second_stage(summary, prompt)\n",
    "    print(\"---- json_llm ----\")\n",
    "    print(json_llm)\n",
    "    json_llm = json_repair.loads(json_llm)\n",
    "    if 'Applicant' in json_llm:\n",
    "        json_llm['KeyPeople'] = json_llm.pop('Applicant')\n",
    "    if 'Assignee' in json_llm:\n",
    "        json_llm['KeyPeople'] = json_llm.pop('Assignee')\n",
    "    if 'Inventor' in json_llm:\n",
    "        json_llm['KeyPeople'] = json_llm.pop('Inventor')\n",
    "    # Run the model evaluator\n",
    "    with open(f\"/scratch/students/ndillenb/metadata/processing/llm/json_compare/de2_gemma-3-4b-it_json_compare/json_llm_{item['_id']}.json\", \"w\") as f:\n",
    "        json.dump({'predicted': json_llm}, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b5d09fd",
   "metadata": {},
   "source": [
    "For french"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4caf612",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "import json_repair\n",
    "import json\n",
    "import random\n",
    "query = {'Country': \"FR\"}\n",
    "training_data = []\n",
    "total_count = collection_txt.count_documents(query)\n",
    "random_indexes = random.sample(range(total_count), 500)\n",
    "for item in tqdm(random_indexes):\n",
    "    item = collection_txt.find_one({'Country': \"FR\"}, skip=item)\n",
    "    #publication_number = item['Publication Number']\n",
    "    if 'OCR' not in item or item['OCR'] is None or item['OCR'] == '':\n",
    "        print(f\"No OCR for {item['Publication Number']}\")\n",
    "        continue\n",
    "    summary = first_stage(item)\n",
    "    print(\"---- summary ----\")\n",
    "    print(summary)\n",
    "    json_llm = second_stage(summary, prompt)\n",
    "    print(\"---- json_llm ----\")\n",
    "    print(json_llm)\n",
    "    json_llm = json_repair.loads(json_llm)\n",
    "    if 'Applicant' in json_llm:\n",
    "        json_llm['KeyPeople'] = json_llm.pop('Applicant')\n",
    "    if 'Assignee' in json_llm:\n",
    "        json_llm['KeyPeople'] = json_llm.pop('Assignee')\n",
    "    if 'Inventor' in json_llm:\n",
    "        json_llm['KeyPeople'] = json_llm.pop('Inventor')\n",
    "    # Run the model evaluator\n",
    "    with open(f\"/scratch/students/ndillenb/metadata/processing/llm/json_compare/fr_gemma-3-4b-it_json_compare/json_llm_{item['_id']}.json\", \"w\") as f:\n",
    "        json.dump({'predicted': json_llm}, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7edf4dbe",
   "metadata": {},
   "source": [
    "For json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9574f9d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "import json_repair\n",
    "import json\n",
    "for item in tqdm(list(collection_json.find({'Country':\"US\", 'OCR': {'$exists': True}, 'Title': {'$exists': True}, 'C_Application Date': {'$exists': True}, 'C_Publication Date': {'$exists': True}, 'clean_applicants': {'$exists': True}, 'clean_inventor': {'$exists': True}}).limit(100))):\n",
    "    summary = first_stage(item)\n",
    "    print(\"---- summary ----\")\n",
    "    print(summary)\n",
    "    json_llm = second_stage(summary, prompt)\n",
    "    print(\"---- json_llm ----\")\n",
    "    print(json_llm)\n",
    "    json_llm = json_repair.loads(json_llm)\n",
    "    # Run the model evaluator\n",
    "    with open(f\"/scratch/students/ndillenb/metadata/processing/llm/json_compare/gemma-3-4b-it_json_compare/json_llm_{item['_id']}.json\", \"w\") as f:\n",
    "        data = {'Title': item['Title'], 'Application_Date': item['C_Application Date'], 'Publication_Date': item['C_Publication Date'], 'Applicants': item['clean_applicants'], 'Inventors': item['clean_inventor']}\n",
    "        # Convert datetime objects to strings for JSON serialization\n",
    "        data_serializable = {\n",
    "            key: (value.isoformat() if isinstance(value, datetime.datetime) else value)\n",
    "            for key, value in data.items()\n",
    "        }\n",
    "        json.dump({'predicted': json_llm, 'expected': data_serializable}, f)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "metadata",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
