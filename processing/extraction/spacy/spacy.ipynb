{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ee073d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pymongo import UpdateOne\n",
    "from tqdm import tqdm\n",
    "from pymongo import MongoClient\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from datetime import datetime\n",
    "import re\n",
    "client = MongoClient(\"localhost\", 29012)\n",
    "db = client[\"test-database\"]\n",
    "collection_json = db[\"collection-json\"]\n",
    "def generate_query(item):\n",
    "    text_clean = item['OCR'].replace('\\n', ' ').strip()\n",
    "    return text_clean\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69867f72",
   "metadata": {},
   "source": [
    "## Train spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71220b0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def overlap(first, second):\n",
    "    start1, end1 = first\n",
    "    start2, end2 = second\n",
    "    if start1 < start2:\n",
    "        return end1 > start2\n",
    "    else:\n",
    "        return start1 < end2\n",
    "\n",
    "def is_overlapping(first, second):\n",
    "    start1, end1 = first\n",
    "    start2, end2 = second\n",
    "    return max(start1, start2) < min(end1, end2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "025649f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def custom_ratio(s1,\n",
    "    s2,\n",
    "    *,\n",
    "    processor=None,\n",
    "    score_cutoff=None,\n",
    "):\n",
    "    ratio = fuzz.token_sort_ratio(s1, s2, processor=processor, score_cutoff=score_cutoff)\n",
    "    difference = abs(len(s1) - len(s2))\n",
    "    print(f\"Length difference: {difference}\")\n",
    "    ratio = ratio - (difference * 0.1)\n",
    "    return ratio if ratio > 0 else 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b595a594",
   "metadata": {},
   "outputs": [],
   "source": [
    "from rapidfuzz import fuzz\n",
    "def reduce_string(string, name, start):\n",
    "    while not string[0].isalnum():\n",
    "        string = string[1:]\n",
    "        start += 1\n",
    "    while not string[-1].isalnum():\n",
    "        string = string[:-1]\n",
    "    tokens = string.split()\n",
    "    score = fuzz.token_sort_ratio(string, name)\n",
    "    i=0\n",
    "    max_score = score\n",
    "    while i<len(tokens) and fuzz.token_sort_ratio(' '.join(tokens[i:]), name) >= max_score:\n",
    "        max_score = fuzz.token_sort_ratio(' '.join(tokens[i:]), name)\n",
    "        i += 1\n",
    "    if i>0:\n",
    "        i -= 1\n",
    "    start = start + len(' '.join(tokens[:i])) + 1\n",
    "    j=len(tokens)\n",
    "    while j>0 and fuzz.token_sort_ratio(' '.join(tokens[i:j]), name) >= max_score:\n",
    "        max_score = fuzz.token_sort_ratio(' '.join(tokens[i:j]), name)\n",
    "        j -= 1\n",
    "    if j<len(tokens):\n",
    "        j += 1\n",
    "\n",
    "    return ' '.join(tokens[i:j]), start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e425b6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Testing window matching with fuzzy\n",
    "text_lower = \"The inventor is Capable of many things, Mark James Henry, is the inventor and he has been doing a lot about this issue. Mark Henry is really pationate about this whole thing. You know him? Mark HENRY?\"\n",
    "import re\n",
    "name = \"mark henry\"\n",
    "matches = process.extract(\n",
    "    name,\n",
    "    [text_lower[i:i+len(\"mark henry\")+10] for i in range(len(text_lower))],\n",
    "    scorer=fuzz.token_sort_ratio,\n",
    "    processor=utils.default_process,\n",
    "    limit=10\n",
    ")\n",
    "\n",
    "matches_updated = []\n",
    "for match in matches:\n",
    "    print(match)\n",
    "    new_match, start = reduce_string(match[0], name, match[2])\n",
    "    difference = abs(len(name) - len(new_match))\n",
    "    ratio = match[1] - (difference * 0.1)\n",
    "    matches_updated.append((new_match, ratio, start))\n",
    "print(matches_updated)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92eb3058",
   "metadata": {},
   "outputs": [],
   "source": [
    "from rapidfuzz import process, fuzz, utils\n",
    "import re\n",
    "def find_fuzzy_matches(text, name_list, label, entities, threshold=70):\n",
    "    text_lower = text.lower()\n",
    "    words_in_text = text.split()\n",
    "    for name in name_list:\n",
    "        name_lower = name.lower()\n",
    "\n",
    "        # Use fuzzy matching to find the closest substring(s)\n",
    "        matches = process.extract(\n",
    "            name_lower,\n",
    "            [text_lower[i:i+len(name_lower)+10] for i in range(len(text_lower))],\n",
    "            scorer=fuzz.token_sort_ratio,\n",
    "            processor=utils.default_process,\n",
    "            limit=20\n",
    "        )\n",
    "        matches_updated = []\n",
    "        for match in matches:\n",
    "            new_match, start = reduce_string(match[0], name_lower, match[2])\n",
    "            difference = abs(len(name) - len(new_match))\n",
    "            ratio = match[1] - (difference * 0.2)\n",
    "            matches_updated.append((new_match, ratio, start))\n",
    "        matches = matches_updated\n",
    "        for match_str, score, match_start in matches:\n",
    "            if score < threshold:\n",
    "                continue\n",
    "            actual_end = match_start + len(match_str)\n",
    "            actual_start = match_start\n",
    "            while actual_start > 0 and actual_start<actual_end and actual_start < len(text_lower):\n",
    "                if not text[actual_start].isalnum():\n",
    "                    actual_start+=1\n",
    "                elif text[actual_start-1].isalnum():\n",
    "                    actual_start -= 1\n",
    "                elif text[actual_start].isalnum() and not text[actual_start-1].isalnum():\n",
    "                    break\n",
    "                else:\n",
    "                    break\n",
    "            while actual_end > 0 and actual_end>actual_start and actual_end < len(text_lower):\n",
    "                if not text[actual_end].isalnum():\n",
    "                    actual_end-=1\n",
    "                elif actual_end+1 < len(text_lower) and text[actual_end+1].isalnum():\n",
    "                    actual_end += 1\n",
    "                elif text[actual_end].isalnum() and actual_end+1 < len(text_lower) and not text[actual_end+1].isalnum():\n",
    "                    actual_end += 1\n",
    "                    break\n",
    "                else:\n",
    "                    actual_end += 1\n",
    "                    break\n",
    "            score = fuzz.token_sort_ratio(text[actual_start:actual_end], name)\n",
    "            # Prevent overlap: prefer longer matches\n",
    "            should_add = True\n",
    "            overlapping = []\n",
    "            actual_pos = (actual_start, actual_end)\n",
    "            for already_found in entities:\n",
    "                already_found_pos = (already_found[0], already_found[1])\n",
    "                if is_overlapping(already_found_pos, actual_pos):\n",
    "                    overlapping.append(already_found)\n",
    "            if all(score > o[3] for o in overlapping):\n",
    "                # Remove all overlapping lower-score entities\n",
    "                for o in overlapping:\n",
    "                    entities.remove(o)\n",
    "                entities.append((actual_start, actual_end, label, score))\n",
    "            else:\n",
    "                # Do not add the new one\n",
    "                pass\n",
    "    return entities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea5702f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def date_variations(date):\n",
    "    if isinstance(date, str):\n",
    "        date = datetime.strptime(date, '%Y-%m-%d')\n",
    "    date_variations = []\n",
    "    date_variations.append(date.strftime('%Y-%m-%d'))\n",
    "    date_variations.append(date.strftime('%d-%m-%Y'))\n",
    "    date_variations.append(date.strftime('%m-%d-%Y'))\n",
    "    date_variations.append(date.strftime('%d %B %Y'))\n",
    "    date_variations.append(date.strftime('%B %d, %Y'))\n",
    "    date_variations.append(date.strftime('%B %d %Y'))\n",
    "    date_variations.append(date.strftime('%d %b %Y'))\n",
    "    date_variations.append(date.strftime('%b %d, %Y'))\n",
    "    date_variations.append(date.strftime('%b %d %Y'))\n",
    "    date_variations.append(date.strftime('%d %B %y'))\n",
    "    date_variations.append(date.strftime('%B %d, %y'))\n",
    "    date_variations.append(date.strftime('%B %d %y'))\n",
    "    date_variations.append(date.strftime('%d %b %y'))\n",
    "    date_variations.append(date.strftime('%b %d, %y'))\n",
    "    date_variations.append(date.strftime('%b %d %y'))\n",
    "    date_variations.append(date.strftime('%d %m %Y'))\n",
    "    date_variations.append(date.strftime('%m %d %Y'))\n",
    "    date_variations.append(date.strftime('%d %m %y'))\n",
    "    date_variations.append(date.strftime('%m %d %y'))\n",
    "    date_variations.append(date.strftime('%d %m %Y'))\n",
    "    date_variations.append(date.strftime('%m %d %Y'))\n",
    "    date_variations.append(date.strftime('%d %m %y'))\n",
    "    date_variations.append(date.strftime('%m %d %y'))\n",
    "    return date_variations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "271efeeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "for item in tqdm(list(collection_json.find({'Country':\"US\", 'OCR': {'$exists': True}, 'Title': {'$exists': True}, 'C_Application Date': {'$exists': True}, 'C_Publication Date': {'$exists': True}}).limit(5000))):\n",
    "    print(item['C_Application Date'])\n",
    "    print(type(item['C_Application Date']))\n",
    "    print(date_variations(item['C_Application Date']))\n",
    "    print(item['C_Publication Date'])\n",
    "    print(type(item['C_Publication Date']))\n",
    "    print(date_variations(item['C_Publication Date']))\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27ce12e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "training_data = []\n",
    "for item in tqdm(list(collection_json.find({'Country':\"US\", 'OCR': {'$exists': True}, 'Title': {'$exists': True}, 'C_Application Date': {'$exists': True}, 'C_Publication Date': {'$exists': True}}).limit(5000))):\n",
    "    text = generate_query(item)\n",
    "    entities = []\n",
    "    training_data.append((text, {'entities':entities}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a70026d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "for element in training_data:\n",
    "    entities = element[1]['entities']\n",
    "    for item1 in entities:\n",
    "        for item2 in entities:\n",
    "            if item1 != item2:\n",
    "                if item1[2] == item2[2] and is_overlapping((item1[0], item1[1]), (item2[0], item2[1])):\n",
    "                    print(f\"Found overlapping entities: {item1} and {item2} with text {element[0][item1[0]:item1[1]]} and {element[0][item2[0]:item2[1]]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62a15f04",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "random.shuffle(training_data)\n",
    "train_data = training_data[:int(len(training_data) * 0.7)]\n",
    "dev_data = training_data[int(len(training_data) * 0.7):]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96c223a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "from tqdm import tqdm\n",
    "from spacy.tokens import DocBin\n",
    "\n",
    "nlp = spacy.blank(\"en\")  # start with a blank English model\n",
    "db = DocBin()\n",
    "\n",
    "for text, annot in tqdm(train_data):\n",
    "    try:\n",
    "        doc = nlp.make_doc(text)\n",
    "        ents = []\n",
    "        for start, end, label, score in annot[\"entities\"]:\n",
    "            span = doc.char_span(start, end, label=label)\n",
    "            if span is None:\n",
    "                print(f\"Skipping: {text[start-10:start]}${text[start:end]}${text[end:end+10]} — invalid span\")\n",
    "            else:\n",
    "                ents.append(span)\n",
    "        if len(ents)!=0:\n",
    "            doc.ents = ents\n",
    "            db.add(doc)\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing text {e}\")\n",
    "\n",
    "db.to_disk(\"./train4.spacy\")\n",
    "\n",
    "nlp = spacy.blank(\"en\")  # start with a blank English model\n",
    "db = DocBin()\n",
    "\n",
    "for text, annot in tqdm(dev_data):\n",
    "    try:\n",
    "        doc = nlp.make_doc(text)\n",
    "        ents = []\n",
    "        for start, end, label, score in annot[\"entities\"]:\n",
    "            span = doc.char_span(start, end, label=label)\n",
    "            if span is None:\n",
    "                print(f\"Skipping: {text[start-10:start]}${text[start:end]}${text[end:end+10]} — invalid span\")\n",
    "                #print(f\"Start: {start}, End: {end}, Label: {label}\")\n",
    "            else:\n",
    "                ents.append(span)\n",
    "        if len(ents)!=0:\n",
    "            doc.ents = ents\n",
    "            db.add(doc)\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing text {e}\")\n",
    "\n",
    "db.to_disk(\"./dev4.spacy\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a1e8d27",
   "metadata": {},
   "source": [
    "Training in the command line"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a8a9c17",
   "metadata": {},
   "outputs": [],
   "source": [
    "!python -m spacy init config config.cfg --lang en --pipeline ner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eaa9ef3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "!python -m spacy train config.cfg --output ./output4 --paths.train ./train4.spacy --paths.dev ./dev4.spacy --gpu-id 0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20b12335",
   "metadata": {},
   "source": [
    "Load and execute the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cef48610",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "import json\n",
    "from rapidfuzz import process, fuzz, utils\n",
    "predict_dir = \"/scratch/students/ndillenb/metadata/processing/llm/json_compare/spacy_en_json_compare/\"\n",
    "nlp = spacy.load(\"/scratch/students/ndillenb/metadata/processing/ocr_extract/spacy_files/output4/model-best\")\n",
    "for item in tqdm(list(collection_json.find({'Country':\"US\", 'OCR': {'$exists': True}, 'Title': {'$exists': True}, 'C_Application Date': {'$exists': True}, 'C_Publication Date': {'$exists': True}}).limit(1000))):\n",
    "    text = generate_query(item)\n",
    "    names = []\n",
    "    doc = nlp(text)\n",
    "    predicted = {\n",
    "        \"Title\":None,\n",
    "        \"Application_Date\":None,\n",
    "        \"Publication_Date\":None,\n",
    "        \"Applicants\":[],\n",
    "    }\n",
    "\n",
    "    for ent in doc.ents:\n",
    "        if ent.label_ == \"TITLE\":\n",
    "            predicted[\"title\"] = ent.text\n",
    "        elif ent.label_ == \"APPLICATION_DATE\":\n",
    "            predicted[\"Application_Date\"] = ent.text\n",
    "        elif ent.label_ == \"PUBLICATION_DATE\":\n",
    "            predicted[\"Publication_Date\"] = ent.text\n",
    "        elif ent.label_ == \"APPLICANT\":\n",
    "                if not any(fuzz.token_sort_ratio(ent.text.lower(), name.lower()) >= 70 for name in predicted[\"Applicants\"]):\n",
    "                    predicted[\"Applicants\"].append(ent.text)\n",
    "        print(f\"Entity: {ent.text}, Label: {ent.label_}\")\n",
    "    with open(os.path.join(predict_dir, f\"json_llm_{item['_id']}.json\"), 'w') as f:\n",
    "        json.dump({'predicted':predicted}, f)\n",
    "    print('---')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6d4f932",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "from rapidfuzz import process, fuzz, utils\n",
    "nlp = spacy.load(\"./output/model-best\")\n",
    "for item in tqdm(list(collection_json.find({'Country':\"US\", 'OCR': {'$exists': True}, 'Title': {'$exists': True}, 'C_Application Date': {'$exists': True}, 'C_Publication Date': {'$exists': True}}).limit(100))):\n",
    "    text = generate_query(item)\n",
    "    names = []\n",
    "    if item['Applicant'] is not None and item['Applicant'] != []:\n",
    "        for name in item['Applicant']:\n",
    "            names.append(name)\n",
    "    if item['Inventor'] is not None and item['Inventor'] != []:\n",
    "        for name in item['Inventor']:\n",
    "            names.append(name)\n",
    "    unique_named = []\n",
    "    for name in names:\n",
    "        if not any(fuzz.token_sort_ratio(name, existing_name) > 70 for existing_name in unique_named):\n",
    "            unique_named.append(name)\n",
    "    names = unique_named\n",
    "    print(names)\n",
    "    doc = nlp(text)\n",
    "    names_matches = [0 for i in range(len(names))]\n",
    "    for ent in doc.ents:\n",
    "        for knownames in names:\n",
    "            if fuzz.token_sort_ratio(ent.text.lower(), knownames.lower()) > 70:\n",
    "                print(f\"Found a match: {ent.text} with {knownames}\")\n",
    "                names_matches[names.index(knownames)] += 1                \n",
    "    for i in range(len(names)):\n",
    "        if names_matches[i] == 0:\n",
    "            print(f\"❌ Did not find a match for {names[i]}\")\n",
    "        else:\n",
    "            print(f\"✔️ Found a match for {names[i]} with {names_matches[i]}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "metadata",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
