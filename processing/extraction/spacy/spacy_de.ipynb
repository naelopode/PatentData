{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ee073d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pymongo import UpdateOne\n",
    "from tqdm import tqdm\n",
    "from pymongo import MongoClient\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from datetime import datetime\n",
    "import re\n",
    "client = MongoClient(\"localhost\", 29012)\n",
    "db = client[\"test-database\"]\n",
    "collection_txt = db[\"collection-txt2\"]\n",
    "collection_CNN = db[\"CNN_DE\"]\n",
    "\n",
    "def get_text(publication_number):\n",
    "    publication_number = f\"{publication_number[:2]}.{publication_number[2:-1]}.{publication_number[-1:]}\"\n",
    "    pipeline = [\n",
    "        {\"$match\": {\"Publication Number\": publication_number}},  # Filter documents where 'type' is 'text'\n",
    "        {\"$group\": {\n",
    "            \"_id\": \"$Publication Number\",  # Group by 'Publication Number'\n",
    "            \"pages\": {\"$push\": \"$page\"},   # Collect 'page' values into an array\n",
    "            \"text\": {\"$push\": \"$OCR\"}  # Collect 'OCR' values into an array\n",
    "        }}\n",
    "    ]\n",
    "\n",
    "    result_db = list(collection_CNN.aggregate(pipeline))\n",
    "    if not result_db:\n",
    "        return None\n",
    "    result ={\n",
    "        \"Publication Number\": result_db[0][\"_id\"],\n",
    "        \"pages\": result_db[0][\"pages\"],\n",
    "        \"OCR\": ' '.join([item for item in result_db[0][\"text\"]])  # Join the text from all pages\n",
    "    }\n",
    "    return result\n",
    "\n",
    "def generate_query(item):\n",
    "    text_clean = item['OCR'].replace('\\n', ' ').strip()\n",
    "    return text_clean\n",
    "\n",
    "print(get_text(\"DE2C\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69867f72",
   "metadata": {},
   "source": [
    "## Train spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71220b0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def overlap(first, second):\n",
    "    start1, end1 = first\n",
    "    start2, end2 = second\n",
    "    if start1 < start2:\n",
    "        return end1 > start2\n",
    "    else:\n",
    "        return start1 < end2\n",
    "def is_overlapping(first, second):\n",
    "    start1, end1 = first\n",
    "    start2, end2 = second\n",
    "    return max(start1, start2) < min(end1, end2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "025649f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def custom_ratio(s1,\n",
    "    s2,\n",
    "    *,\n",
    "    processor=None,\n",
    "    score_cutoff=None,\n",
    "):\n",
    "    ratio = fuzz.token_sort_ratio(s1, s2, processor=processor, score_cutoff=score_cutoff)\n",
    "    difference = abs(len(s1) - len(s2))\n",
    "    print(f\"Length difference: {difference}\")\n",
    "    ratio = ratio - (difference * 0.1)\n",
    "    return ratio if ratio > 0 else 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b595a594",
   "metadata": {},
   "outputs": [],
   "source": [
    "def reduce_string(string, name, start):\n",
    "    while not string[0].isalnum():\n",
    "        string = string[1:]\n",
    "        start += 1\n",
    "    while not string[-1].isalnum():\n",
    "        string = string[:-1]\n",
    "    tokens = string.split()\n",
    "    score = fuzz.token_sort_ratio(string, name)\n",
    "    i=0\n",
    "    max_score = score\n",
    "    while i<len(tokens) and fuzz.token_sort_ratio(' '.join(tokens[i:]), name) >= max_score:\n",
    "        max_score = fuzz.token_sort_ratio(' '.join(tokens[i:]), name)\n",
    "        i += 1\n",
    "    if i>0:\n",
    "        i -= 1\n",
    "    start = start + len(' '.join(tokens[:i])) + 1\n",
    "    j=len(tokens)\n",
    "    while j>0 and fuzz.token_sort_ratio(' '.join(tokens[i:j]), name) >= max_score:\n",
    "        max_score = fuzz.token_sort_ratio(' '.join(tokens[i:j]), name)\n",
    "        j -= 1\n",
    "    if j<len(tokens):\n",
    "        j += 1\n",
    "\n",
    "    return ' '.join(tokens[i:j]), start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92eb3058",
   "metadata": {},
   "outputs": [],
   "source": [
    "from rapidfuzz import process, fuzz, utils\n",
    "import re\n",
    "def find_fuzzy_matches(text, name_list, label, entities, threshold=70):\n",
    "    text_lower = text.lower()\n",
    "    words_in_text = text.split()\n",
    "    for name in name_list:\n",
    "        name_lower = name.lower()\n",
    "\n",
    "        # Use fuzzy matching to find the closest substring(s)\n",
    "        matches = process.extract(\n",
    "            name_lower,\n",
    "            [text_lower[i:i+len(name_lower)+10] for i in range(len(text_lower))],\n",
    "            scorer=fuzz.token_sort_ratio,\n",
    "            processor=utils.default_process,\n",
    "            limit=20\n",
    "        )\n",
    "        matches_updated = []\n",
    "        for match in matches:\n",
    "            new_match, start = reduce_string(match[0], name_lower, match[2])\n",
    "            difference = abs(len(name) - len(new_match))\n",
    "            ratio = match[1] - (difference * 0.2)\n",
    "            matches_updated.append((new_match, ratio, start))\n",
    "        matches = matches_updated\n",
    "        for match_str, score, match_start in matches:\n",
    "            if score < threshold:\n",
    "                continue\n",
    "            actual_end = match_start + len(match_str)\n",
    "            actual_start = match_start\n",
    "            while actual_start > 0 and actual_start<actual_end and actual_start < len(text_lower):\n",
    "                if not text[actual_start].isalnum():\n",
    "                    actual_start+=1\n",
    "                elif text[actual_start-1].isalnum():\n",
    "                    actual_start -= 1\n",
    "                elif text[actual_start].isalnum() and not text[actual_start-1].isalnum():\n",
    "                    break\n",
    "                else:\n",
    "                    break\n",
    "            #print('___')\n",
    "            while actual_end > 0 and actual_end>actual_start and actual_end < len(text_lower):\n",
    "                if not text[actual_end].isalnum():\n",
    "                    actual_end-=1\n",
    "                elif actual_end+1 < len(text_lower) and text[actual_end+1].isalnum():\n",
    "                    actual_end += 1\n",
    "                elif text[actual_end].isalnum() and actual_end+1 < len(text_lower) and not text[actual_end+1].isalnum():\n",
    "                    actual_end += 1\n",
    "                    break\n",
    "                else:\n",
    "                    actual_end += 1\n",
    "                    break\n",
    "            score = fuzz.token_sort_ratio(text[actual_start:actual_end], name)\n",
    "            # Prevent overlap: prefer longer matches\n",
    "            should_add = True\n",
    "            overlapping = []\n",
    "            actual_pos = (actual_start, actual_end)\n",
    "            for already_found in entities:\n",
    "                already_found_pos = (already_found[0], already_found[1])\n",
    "                if is_overlapping(already_found_pos, actual_pos):\n",
    "                    overlapping.append(already_found)\n",
    "            if all(score > o[3] for o in overlapping):\n",
    "                # Remove all overlapping lower-score entities\n",
    "                for o in overlapping:\n",
    "                    entities.remove(o)\n",
    "                entities.append((actual_start, actual_end, label, score))\n",
    "            else:\n",
    "                # Do not add the new one\n",
    "                pass\n",
    "    return entities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f7d59b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "month_translation = {\n",
    "\t\"January\": \"Januar\",\n",
    "\t\"February\": \"Februar\",\n",
    "\t\"March\": \"März\",\n",
    "\t\"April\": \"April\",\n",
    "\t\"May\": \"Mai\",\n",
    "\t\"June\": \"Juni\",\n",
    "\t\"July\": \"Juli\",\n",
    "\t\"August\": \"August\",\n",
    "\t\"September\": \"September\",\n",
    "\t\"October\": \"Oktober\",\n",
    "\t\"November\": \"November\",\n",
    "\t\"December\": \"Dezember\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea5702f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def date_variations(date):\n",
    "    if isinstance(date, str):\n",
    "        date = datetime.strptime(date, '%Y-%m-%d')\n",
    "    date_variations = []\n",
    "    date_variations.append(date.strftime('%Y-%m-%d'))\n",
    "    date_variations.append(date.strftime('%d-%m-%Y'))\n",
    "    date_variations.append(date.strftime('%m-%d-%Y'))\n",
    "    date_variations.append(date.strftime('%d %B %Y'))\n",
    "    date_variations.append(date.strftime('%B %d, %Y'))\n",
    "    date_variations.append(date.strftime('%B %d %Y'))\n",
    "    date_variations.append(date.strftime('%d %b %Y'))\n",
    "    date_variations.append(date.strftime('%b %d, %Y'))\n",
    "    date_variations.append(date.strftime('%b %d %Y'))\n",
    "    date_variations.append(date.strftime('%d %B %y'))\n",
    "    date_variations.append(date.strftime('%B %d, %y'))\n",
    "    date_variations.append(date.strftime('%B %d %y'))\n",
    "    date_variations.append(date.strftime('%d %b %y'))\n",
    "    date_variations.append(date.strftime('%b %d, %y'))\n",
    "    date_variations.append(date.strftime('%b %d %y'))\n",
    "    date_variations.append(date.strftime('%d %m %Y'))\n",
    "    date_variations.append(date.strftime('%m %d %Y'))\n",
    "    date_variations.append(date.strftime('%d %m %y'))\n",
    "    date_variations.append(date.strftime('%m %d %y'))\n",
    "    date_variations.append(date.strftime('%d %m %Y'))\n",
    "    date_variations.append(date.strftime('%m %d %Y'))\n",
    "    date_variations.append(date.strftime('%d %m %y'))\n",
    "    date_variations.append(date.strftime('%m %d %y'))\n",
    "    for item in date_variations:\n",
    "        for month in month_translation:\n",
    "            if month in item:\n",
    "                item = item.replace(month, month_translation[month])\n",
    "                date_variations.append(item)\n",
    "    return date_variations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27ce12e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "import random\n",
    "query = {'Country': \"DE\", 'Title': {'$exists': True}, 'C_Application Date': {'$exists': True}, 'C_Publication Date': {'$exists': True}, 'Applicant': {'$exists': True}, 'Inventor': {'$exists': True}}\n",
    "training_data = []\n",
    "total_count = collection_txt.count_documents(query)\n",
    "random_indexes = random.sample(range(total_count), 3000)\n",
    "count_skipped = 0\n",
    "for index in tqdm(random_indexes):\n",
    "    item = collection_txt.find(query).skip(index).limit(1)[0]\n",
    "    find_ocr = get_text(item['Publication Number'])\n",
    "    if find_ocr is None:\n",
    "        print(f\"Publication Number {item['Publication Number']} not found\")\n",
    "        count_skipped += 1\n",
    "        continue\n",
    "    text = generate_query(find_ocr)\n",
    "    entities = []\n",
    "    if 'Applicant' in item and item['Applicant'] is not None and item['Applicant'] != []:\n",
    "        entities = find_fuzzy_matches(text, item['Applicant'], \"APPLICANT\", entities)\n",
    "    if 'Inventor' in item and item['Inventor'] is not None and item['Inventor'] != []:\n",
    "        entities = find_fuzzy_matches(text, item['Inventor'], \"APPLICANT\", entities)\n",
    "    if item['Title'] is not None:\n",
    "        entities = find_fuzzy_matches(text, [item['Title']], \"TITLE\", entities)\n",
    "    if item['C_Application Date'] is not None:\n",
    "        application_date = date_variations(item['C_Application Date'])\n",
    "        entities = find_fuzzy_matches(text, application_date, \"APPLICATION_DATE\", entities)\n",
    "    if item['C_Publication Date'] is not None:\n",
    "        publication_date = date_variations(item['C_Publication Date'])\n",
    "        entities = find_fuzzy_matches(text, publication_date, \"PUBLICATION_DATE\", entities)\n",
    "    training_data.append((text, {'entities': entities}))\n",
    "print(f\"Skipped {count_skipped} items\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a70026d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "for element in training_data:\n",
    "    entities = element[1]['entities']\n",
    "    for item1 in entities:\n",
    "        for item2 in entities:\n",
    "            if item1 != item2:\n",
    "                if item1[2] == item2[2] and is_overlapping((item1[0], item1[1]), (item2[0], item2[1])):\n",
    "                    print(f\"Found overlapping entities: {item1} and {item2} with text {element[0][item1[0]:item1[1]]} and {element[0][item2[0]:item2[1]]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62a15f04",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "training_data = training_data\n",
    "random.shuffle(training_data)\n",
    "train_data = training_data[:int(len(training_data) * 0.7)]\n",
    "dev_data = training_data[int(len(training_data) * 0.7):]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96c223a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "from tqdm import tqdm\n",
    "from spacy.tokens import DocBin\n",
    "\n",
    "nlp = spacy.blank(\"de\")  # start with a blank English model\n",
    "db = DocBin()\n",
    "\n",
    "for text, annot in tqdm(train_data):\n",
    "    try:\n",
    "        doc = nlp.make_doc(text)\n",
    "        ents = []\n",
    "        for start, end, label, score in annot[\"entities\"]:\n",
    "            span = doc.char_span(start, end, label=label)\n",
    "            if span is None:\n",
    "                print(f\"Skipping: {text[start-10:start]}${text[start:end]}${text[end:end+10]} — invalid span\")\n",
    "            else:\n",
    "                ents.append(span)\n",
    "        if len(ents)!=0:\n",
    "            doc.ents = ents\n",
    "            db.add(doc)\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing text {e}\")\n",
    "\n",
    "db.to_disk(\"./train_de2.spacy\")\n",
    "\n",
    "nlp = spacy.blank(\"de\")  # start with a blank English model\n",
    "db = DocBin()\n",
    "\n",
    "for text, annot in tqdm(dev_data):\n",
    "    try:\n",
    "        doc = nlp.make_doc(text)\n",
    "        ents = []\n",
    "        for start, end, label, score in annot[\"entities\"]:\n",
    "            span = doc.char_span(start, end, label=label)\n",
    "            if span is None:\n",
    "                print(f\"Skipping: {text[start-10:start]}${text[start:end]}${text[end:end+10]} — invalid span\")\n",
    "                #print(f\"Start: {start}, End: {end}, Label: {label}\")\n",
    "            else:\n",
    "                ents.append(span)\n",
    "        if len(ents)!=0:\n",
    "            doc.ents = ents\n",
    "            db.add(doc)\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing text {e}\")\n",
    "\n",
    "db.to_disk(\"./dev_de2.spacy\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a47c7df",
   "metadata": {},
   "source": [
    "Train model in command line"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c93dc62b",
   "metadata": {},
   "outputs": [],
   "source": [
    "!python -m spacy init config config.cfg --lang de --pipeline ner\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f4710aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "!python -m spacy train config_de.cfg --output ./output_de2 --paths.train ./train_de2.spacy --paths.dev ./dev_de2.spacy --gpu-id 0,1!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cef48610",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "import json\n",
    "import random\n",
    "from rapidfuzz import process, fuzz, utils\n",
    "predict_dir = \"/scratch/students/ndillenb/metadata/processing/llm/json_compare/spacy_de2_json_compare/\"\n",
    "nlp = spacy.load(\"./output_de2/model-best\")\n",
    "query = {'Country': \"DE\"}\n",
    "total_count = collection_txt.count_documents(query)\n",
    "random_indexes = random.sample(range(total_count), 500)\n",
    "for item in tqdm(random_indexes):\n",
    "    item = collection_txt.find(query).skip(item).limit(1)[0]\n",
    "    find_ocr = get_text(item['Publication Number'])\n",
    "    if find_ocr is None:\n",
    "        print(f\"Publication Number {item['Publication Number']} not found\")\n",
    "        continue\n",
    "    text = generate_query(find_ocr)\n",
    "    names = []\n",
    "    doc = nlp(text)\n",
    "    predicted = {\n",
    "        \"Title\":None,\n",
    "        \"Application_Date\":None,\n",
    "        \"Publication_Date\":None,\n",
    "        \"Applicants\":[],\n",
    "    }\n",
    "    for ent in doc.ents:\n",
    "        if ent.label_ == \"TITLE\":\n",
    "            predicted[\"title\"] = ent.text\n",
    "        elif ent.label_ == \"APPLICATION_DATE\":\n",
    "            predicted[\"Application_Date\"] = ent.text\n",
    "        elif ent.label_ == \"PUBLICATION_DATE\":\n",
    "            predicted[\"Publication_Date\"] = ent.text\n",
    "        elif ent.label_ == \"APPLICANT\":\n",
    "                if not any(fuzz.token_sort_ratio(ent.text.lower(), name.lower()) >= 70 for name in predicted[\"Applicants\"]):\n",
    "                    predicted[\"Applicants\"].append(ent.text)\n",
    "        print(f\"Entity: {ent.text}, Label: {ent.label_}\")\n",
    "    with open(os.path.join(predict_dir, f\"json_llm_{item['_id']}.json\"), 'w') as f:\n",
    "        json.dump({'predicted':predicted}, f)\n",
    "    print('---')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "metadata",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
